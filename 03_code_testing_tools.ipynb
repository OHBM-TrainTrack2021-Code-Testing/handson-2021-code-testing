{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_code-testing-tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPIH0V04XAibp0EU9TA3+dD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lindajune/handson-2021-code-testing/blob/main/03_code_testing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSAau81_M-k6"
      },
      "source": [
        "#Exploratory testing (Testing with breakpoints)\n",
        "\n",
        "---\n",
        "There are many ways to test your code. Exploratory testing is a form of testing that is done without a plan. A typical way is to **set breakpoints** to run your code. You set breakpoints wherever you want to pause debugger execution. For example, you may want to see the state of code variables or look at the call stack at a certain breakpoint. \n",
        "\n",
        "Note: example in this secion was adapted from [Jupyter Tips and Tricks](https://chrieke.medium.com/jupyter-tips-and-tricks-994fdddb2057) and [Testing in Python](https://realpython.com/python-testing/#testing-your-code)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxLvohIRNZNP"
      },
      "source": [
        "## Magic command %debug\n",
        "The easiest way to debug a Jupyter notebook is to use the %debug magic command. Whenever you encounter an error or exception, just open a new notebook cell, type %debug and run the cell. This will open a command line where you can test your code and inspect all variables right up to the line that threw the error.\n",
        "**Type “n” and hit Enter to run the next line of code** (The → arrow shows you the current position). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdZDtmbqOo65"
      },
      "source": [
        "### example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SKdgl0UOqr8"
      },
      "source": [
        "number_one = 5\n",
        "number_two = 0\n",
        "result = number_one/number_two"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jad_9DsO4ID"
      },
      "source": [
        "%debug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts8K-qXnP5Km"
      },
      "source": [
        "## iPython debugger\n",
        "iPython debugger is another option. Import it and use set_trace() anywhere in your notebook to create one or multiple breakpoints. When executing a cell, it will stop at the first breakpoint and open the command line for code inspection. You can also set breakpoints in the code of imported modules, but don’t forget to import the debugger in there as well. **Use “c” to continue execution.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tqxRS95BRrX"
      },
      "source": [
        "with ***Jupyter Notebook***, here is the way you want to stop a debugger (%debug or iPython debugger):\n",
        "\n",
        "    Type “n” and hit Enter to run the next line of code. \n",
        "    Use “c” to continue until the next breakpoint. \n",
        "    Type “q” quits the debugger and code execution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HNz374fQJy5"
      },
      "source": [
        "### example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXEgeJgEQLnQ"
      },
      "source": [
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "number_one = 5\n",
        "number_two = 0\n",
        "set_trace()\n",
        "result = number_one+number_two\n",
        "print(\"The result is: \" + str(result))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Aewp_sPW6H"
      },
      "source": [
        "## your turn\n",
        "Try to write a piece of code to apply %debug and/or iPyhon debugger by your own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccqBa3vFPlRD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69tL6ywPMDZ"
      },
      "source": [
        "## Visual debugger \n",
        "If you want to set breakpoints in Python like in MATLAB, some JupyterLab extensions may help. \n",
        "For example, there is [a visual debugger for Jupyter](https://blog.jupyter.org/a-visual-debugger-for-jupyter-914e61716559)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5teJ_hDENIqz"
      },
      "source": [
        "# Automated testing (Unit test framework)\n",
        "\n",
        "---\n",
        "When your code is quite long and integrated with multiple functions, the manual exploratory testing could be overwhelming. This is where automated testing comes in. Automated testing is the execution of your test plan (the parts of your application you want to test, the order in which you want to test them, and the expected responses) by a script instead of a human. A typical way of automated testing is the **unit test**.\n",
        "\n",
        "[Unit test framework](https://www.oreilly.com/library/view/unit-test-frameworks/0596006896/ch01.html) are software tools to support writing and running unit tests, including a foundation on which to build tests and the functionality to execute the tests and report their results. A unit test is a smaller test, one that checks that a single component operates in the right way. A unit test helps you to isolate what is broken in your application and fix it faster.\n",
        "\n",
        "There are many unit test runners available for Python. The one built into the Python standard library is called unittest. \n",
        "\n",
        "Note: example in this secion was adapted from [Unittest](https://docs.python.org/3/library/unittest.html), [Quick example for pytest](https://docs.pytest.org/en/latest/) and [ipytest - Unit tests in IPython notebooks](https://github.com/chmp/ipytest)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGk_Lel8NYta"
      },
      "source": [
        "## unittest\n",
        "unittest has been built into the Python standard library since version 2.1. unittest contains both a testing framework and a test runner. unittest has some important requirements for writing and executing tests:\n",
        "\n",
        "    You put your tests into classes as methods\n",
        "    You use a series of special assertion methods in the unittest.TestCase class instead of the built-in assert statement\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94i5GKuzBzO6"
      },
      "source": [
        "NOTE: colab is accumulating the tests, you could use ***Restart runtime*** to avoide it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIJxz6exVvrP"
      },
      "source": [
        "reference: \n",
        "- more about **unittest** could be found [here](https://docs.python.org/3/library/unittest.html)\n",
        "- list of **assert methods** is [here](https://docs.python.org/3/library/unittest.html#assert-methods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-OEJddWZ-eZ"
      },
      "source": [
        "### example 1:\n",
        "This is an example of successful testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB9u2zMLZ86K",
        "outputId": "c756c50b-d59b-474c-c8dd-1a97eb45841a"
      },
      "source": [
        "# Import unittest from the standard library\n",
        "import unittest\n",
        "\n",
        "# Create a class called TestSum that inherits from the TestCase class\n",
        "class TestSum(unittest.TestCase):\n",
        "\n",
        "    # Convert the test functions into methods by adding self as the first argument\n",
        "    # Change the assertions to use the self.assertEqual() method on the TestCase class\n",
        "    def test_sum(self):\n",
        "        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n",
        "\n",
        "# Change the command-line entry point to call unittest.main()\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False) # this is an adaption for colab\n",
        "#     unittest.main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHpwylB-eN9C"
      },
      "source": [
        "### example 2:\n",
        "This is an example of an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQtXAiQGePYz",
        "outputId": "e70e2ee0-d439-4a52-afe9-ea4113e8b479"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class TestSum(unittest.TestCase):\n",
        "\n",
        "    def test_sum(self):\n",
        "        self.assertEqual(sum([1, 2, 3]), 5, \"Should be 6\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_sum (__main__.TestSum)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-7-a5476a1f02d5>\", line 6, in test_sum\n",
            "    self.assertEqual(sum([1, 2, 3]), 5, \"Should be 6\")\n",
            "AssertionError: 6 != 5 : Should be 6\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.005s\n",
            "\n",
            "FAILED (failures=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaafxovWa8-O"
      },
      "source": [
        "### example 3:\n",
        "This is an example to try different assert methods. Please restart runtime to run this example!\n",
        "\n",
        "The TestCase class provides several assert methods to check for and report failures. The table [here](https://docs.python.org/3/library/unittest.html#assert-methods) lists the most commonly used **Assert Methods**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZBZPq2MBohn"
      },
      "source": [
        "## restart runtime to avoid accumulating tests\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb20KF0abIAr",
        "outputId": "1bd6e3ec-e250-413e-eb5f-f2242bd8e590"
      },
      "source": [
        "import unittest\n",
        "class TestAddertMethods(unittest.TestCase):\n",
        "\n",
        "    def test_upper(self):\n",
        "        self.assertEqual('foo'.upper(), 'FOO')\n",
        "\n",
        "    def test_isupper(self):\n",
        "        self.assertTrue('FOO'.isupper())\n",
        "        self.assertFalse('Foo'.isupper())\n",
        "\n",
        "    def test_member(self):\n",
        "        self.assertIn(1, [1, 2, 3])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.009s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hogD-UlJg0Gm"
      },
      "source": [
        "### quiz\n",
        "please complete the code below:\n",
        "- define functions to conduct \"add, subtract, multiply and divide\"\n",
        "- test your functions with unittest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2p_X-lCKHL"
      },
      "source": [
        "## restart runtime to avoid accumulating tests\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzFGZu16g1H1"
      },
      "source": [
        "## define functions\n",
        "# add\n",
        "def add(a, b):\n",
        "    return a+b\n",
        "# subtract\n",
        "# ...\n",
        "\n",
        "## apply unittest\n",
        "import unittest\n",
        "class TestMathFunc(unittest.TestCase):\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fczs5l7ZVHi7"
      },
      "source": [
        "## pytest\n",
        "- [pytest](https://docs.pytest.org/en/latest/) also supports execution of unittest test cases. The real advantage of pytest comes by writing pytest test cases. \n",
        "pytests uses the following [conventions](https://docs.pytest.org/en/6.2.x/goodpractices.html) to automatically discovering tests:\n",
        "  1. files with tests should be called `test_*.py` or `*_test.py `\n",
        "  2. test function name should start with `test_`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg1DbGd7yswb"
      },
      "source": [
        "### example\n",
        "- install `pytest` to the latest version\n",
        "- install `pytest-sugar` if you want nicer output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o21SjpCGyv0P"
      },
      "source": [
        "## OPTIONAL\n",
        "\n",
        "# %pip -q install pytest\n",
        "# %pip -q install pytest-sugar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va7JlHEM67-N"
      },
      "source": [
        "1. to test code, use the `assert` python keyword. pytest adds hooks to assertions to make them more useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9acpZigRVANF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ceb756-24d0-4f22-8426-3b440c92eb1f"
      },
      "source": [
        "%%file test_math.py\n",
        "\n",
        "import math\n",
        "def test_add():\n",
        "    assert 1+1 == 2\n",
        "\n",
        "def test_mul():\n",
        "    assert 6*7 == 42\n",
        "\n",
        "def test_sin():\n",
        "    assert math.sin(0) == 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing test_math.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihPZwTyMDKVm"
      },
      "source": [
        "## To avoide overwrite *.py, you could clean cleanup all files\n",
        "# %rm *.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VexFoEM28CkQ"
      },
      "source": [
        "2.  run pytest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP2SnDmlyv2m",
        "outputId": "9539ec31-82d4-42ae-dab6-ea4320a8e0a5"
      },
      "source": [
        "!python -m pytest test_math.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
            "rootdir: /content\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_math.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkW23FCnZg-S"
      },
      "source": [
        "### more\n",
        "If you have more time, try out additional examples [here](https://colab.research.google.com/github/aviadr1/learn-advanced-python/blob/master/content/08_test_driven_development/08-pytest.ipynb#scrollTo=F8M63k6b6F12)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSaBvpL-yiWS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBqfvEuAZghG"
      },
      "source": [
        "## ipytest\n",
        "Similar to pytest, [**ipytest**](https://pypi.org/project/ipytest/) is designed to enable running tests within an interactive notebook session​. i.e., running unit tests (pytest) inside notebooks\n",
        "\n",
        "At its core, it offers a way to run pytest tests inside the notebook environment. It is also designed to make the transfer of the tests into proper python modules easy by supporting to use standard pytest features.\n",
        "\n",
        "To get started install ipytest via:\n",
        "\n",
        "- pip install -U ipytest\n",
        "\n",
        "To use ipytest, import it and configure the notebook. In most cases, running ipytest.autoconfig() will result in reasonable defaults:\n",
        "\n",
        "- Tests can be executed with the %run_pytest and %run_pytest[clean] magics\n",
        "- The pytest assert rewriting system to get nice assert messages will integrated into the notebook\n",
        "- If not notebook name is given, a workaround using temporary files will be used\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9_N106U2UIQ"
      },
      "source": [
        "### example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-DY8Xbw3thH"
      },
      "source": [
        "- install `ipytest` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vku4r_H2KMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63af82c1-76c2-4d7f-ab52-9e8a6367b097"
      },
      "source": [
        "%pip install -U ipytest"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: ipytest in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pytest>=5.4 in /usr/local/lib/python3.7/dist-packages (from ipytest) (6.2.4)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (21.2.0)\n",
            "Requirement already satisfied, skipping upgrade: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<1.0.0a1,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (0.13.1)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipytest) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.4->ipytest) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.4->ipytest) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnQv1uHf59z2"
      },
      "source": [
        "- import ipytest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtkGegLS3-9V"
      },
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBft1Yzk6dWD"
      },
      "source": [
        "- execute tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENVp40Kz6MW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f833b011-6075-4702-b2fb-3884a43f5451"
      },
      "source": [
        "%%run_pytest[clean]\n",
        "\n",
        "# define the tests\n",
        "def test_my_func():\n",
        "    assert my_func(0) == 0\n",
        "    assert my_func(1) == 0\n",
        "    assert my_func(2) == 2\n",
        "    assert my_func(3) == 2\n",
        "    \n",
        "# the function to be tested\n",
        "def my_func(x):\n",
        "    return x // 2 * 2\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".                                                                        [100%]\n",
            "1 passed in 0.01s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aUJRo1E62L8"
      },
      "source": [
        "### more\n",
        "If you have more time, try out additional examples [here](https://github.com/chmp/ipytest)"
      ]
    }
  ]
}